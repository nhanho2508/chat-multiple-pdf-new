import os
from PyPDF2 import PdfReader
from langchain.text_splitter import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores.faiss import FAISS
from langchain.memory.buffer import ConversationBufferMemory
from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain
from docx import Document
import pytesseract
from PIL import Image
import openpyxl
from langchain.schema import SystemMessage, HumanMessage

VECTORSTORE_PATH = "faiss_index"


def get_text_from_word(doc):
    """
    ƒê·ªçc vƒÉn b·∫£n t·ª´ t√†i li·ªáu Word.
    """
    doc = Document(doc)
    text = ""
    for para in doc.paragraphs:
        text += para.text + "\n"
    return text

def get_text_from_excel(doc):
    """
    ƒê·ªçc vƒÉn b·∫£n t·ª´ t·ªáp Excel.
    """
    wb = openpyxl.load_workbook(doc)
    sheet = wb.active
    text = ""
    for row in sheet.iter_rows(values_only=True):
        text += " ".join([str(cell) for cell in row]) + "\n"
    return text

def get_text_from_image(image_file):
    """
    Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n trong h√¨nh ·∫£nh th√†nh text b·∫±ng OCR.
    """
    image = Image.open(image_file)
    text = pytesseract.image_to_string(image)
    return text

def get_pdf_text(pdf_docs):
    """
    ƒê·ªçc v√† tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ PDF, Word, Excel v√† H√¨nh ·∫£nh.
    """
    text = ""
    for doc in pdf_docs:
        if doc.type == "application/pdf":
            pdf_reader = PdfReader(doc)
            for page in pdf_reader.pages:
                text += page.extract_text()
        elif doc.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            text += get_text_from_word(doc)
        elif doc.type == "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet":
            text += get_text_from_excel(doc)
        elif doc.type in ["image/png", "image/jpeg"]:
            text += get_text_from_image(doc)
    return text

def get_text_chunks(text):
    """
    Chia n·ªôi dung vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n nh·ªè ƒë·ªÉ x·ª≠ l√Ω d·ªÖ d√†ng h∆°n.

    Args:
        text (str): N·ªôi dung vƒÉn b·∫£n c·∫ßn chia nh·ªè.

    Returns:
        list: Danh s√°ch c√°c ƒëo·∫°n vƒÉn b·∫£n nh·ªè.
    """
    text_splitter = CharacterTextSplitter(
        separator="\n",  # NgƒÉn c√°ch c√°c ƒëo·∫°n b·∫±ng d·∫•u xu·ªëng d√≤ng
        chunk_size=1000,  # K√≠ch th∆∞·ªõc m·ªói ƒëo·∫°n
        chunk_overlap=200,  # ƒê·ªô ch·ªìng l·∫•n gi·ªØa c√°c ƒëo·∫°n ƒë·ªÉ gi·ªØ ng·ªØ c·∫£nh
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    return chunks

def get_vector_store(text_chunks, open_ai_keys):
    """
    Ki·ªÉm tra n·∫øu ƒë√£ c√≥ FAISS index l∆∞u tr·ªØ, th√¨ t·∫£i l·∫°i.
    N·∫øu ch∆∞a c√≥, t·∫°o m·ªõi v√† l∆∞u l·∫°i ƒë·ªÉ s·ª≠ d·ª•ng sau n√†y.
    """
    embeddings = OpenAIEmbeddings(
        base_url="https://models.inference.ai.azure.com",
        api_key=open_ai_keys,
        model="text-embedding-3-large"
    )

    # Ki·ªÉm tra n·∫øu index ƒë√£ t·ªìn t·∫°i
    if os.path.exists(f"{VECTORSTORE_PATH}/index"):
        print("‚úÖ Loading existing FAISS index...")
        vectorstore = FAISS.load_local(VECTORSTORE_PATH, embeddings)
    else:
        print("‚ö†Ô∏è No existing FAISS index found. Creating a new one...")
        vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)
        vectorstore.save_local(VECTORSTORE_PATH)  # L∆∞u l·∫°i FAISS index

    return vectorstore

def get_conversation_chain(vectorstore, open_ai_keys):
    """
    T·∫°o chu·ªói h·ªôi tho·∫°i b·∫±ng m√¥ h√¨nh ng√¥n ng·ªØ v√† vector store ƒë·ªÉ x·ª≠ l√Ω truy v·∫•n.

    Args:
        vectorstore (FAISS): Vector store ch·ª©a d·ªØ li·ªáu embeddings.
        open_ai_keys (str): API key c·ªßa OpenAI ƒë·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh ChatGPT.

    Returns:
        ConversationalRetrievalChain: Chu·ªói h·ªôi tho·∫°i v·ªõi tr√≠ nh·ªõ ng·ªØ c·∫£nh.
    """
    llm = ChatOpenAI(
        openai_api_base="https://models.inference.ai.azure.com",
        openai_api_key=open_ai_keys,
        model_name="gpt-4o-mini",
        temperature=1,
        max_tokens=4096
    )

    memory = ConversationBufferMemory(
        memory_key='chat_history', return_messages=True  # L∆∞u tr·ªØ l·ªãch s·ª≠ h·ªôi tho·∫°i
    )
    
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(),  # L·∫•y d·ªØ li·ªáu t·ª´ vector store
        memory=memory
    )
    return conversation_chain

def summarize_text_from_documents(documents, open_ai_keys):
    """
    T√≥m t·∫Øt n·ªôi dung t√†i li·ªáu b·∫±ng c√°ch s·ª≠ d·ª•ng OpenAI GPT.
    """
    llm = ChatOpenAI(
        openai_api_base="https://models.inference.ai.azure.com",
        openai_api_key=open_ai_keys,
        model_name="gpt-4o-mini",
        temperature=1,
        max_tokens=4096
    )

    # K·∫øt h·ª£p n·ªôi dung t·ª´ t·∫•t c·∫£ c√°c t√†i li·ªáu th√†nh m·ªôt vƒÉn b·∫£n l·ªõn
    combined_text = "\n\n".join([doc['text'] for doc in documents])

    # ƒê·ªãnh d·∫°ng tin nh·∫Øn ƒë·ªÉ t√≥m t·∫Øt n·ªôi dung
    messages = [
        SystemMessage(content="You are a helpful assistant."),
        HumanMessage(content=f"Summarize the following text: {combined_text}")
    ]

    # G·ªçi m√¥ h√¨nh ChatOpenAI ƒë·ªÉ t√≥m t·∫Øt vƒÉn b·∫£n
    response = llm.invoke(messages)

    # Tr·∫£ v·ªÅ n·ªôi dung t√≥m t·∫Øt t·ª´ k·∫øt qu·∫£
    summarized_text = response.content
    return summarized_text


def save_feedback(feedback_data, filename="feedback.json"):
    """
    Save feedback data to a JSON file for persistence.
    """
    try:
        with open(filename, "a") as f:
            json.dump(feedback_data, f)
            f.write("\n")  # To separate each feedback entry by a new line.
    except Exception as e:
        print(f"Error saving feedback: {e}")

def handle_user_feedback(response_message):
    """
    Collect user feedback on the quality and relevance of a response.
    """
    thumbs_up = st.button("üëç Thumbs Up")
    thumbs_down = st.button("üëé Thumbs Down")

    if thumbs_up:
        feedback = {"feedback": "positive", "message": response_message}
        st.session_state.feedback.append(feedback)
        save_feedback(feedback)
        st.success("Thank you for your positive feedback!")
    elif thumbs_down:
        feedback = {"feedback": "negative", "message": response_message}
        st.session_state.feedback.append(feedback)
        save_feedback(feedback)
        st.success("Thank you for your feedback! We'll work on improving the answers.")


def adjust_relevance_based_on_feedback(vectorstore, feedback_file="feedback.json"):
    """
    Adjust document relevance in the vectorstore based on accumulated feedback.
    """
    try:
        with open(feedback_file, 'r') as f:
            feedback_data = [json.loads(line) for line in f.readlines()]
        
        # Process feedback and adjust vectorstore accordingly
        # For example, increase the weight of documents marked as "positive" feedback
        for feedback in feedback_data:
            if feedback["feedback"] == "positive":
                # Adjust vectorstore based on positive feedback (this is a simplified example)
                # In practice, you'd use the feedback to fine-tune document embeddings or retrieval strategy.
                pass
    except Exception as e:
        print(f"Error adjusting relevance: {e}")